{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "TlGL52OdzSlj",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Problem 18: COVID-19 USA (total value: 11 points)\n",
    "\n",
    "_Version 1.1b_\n",
    "\n",
    "This problem is a data cleaning and analysis task that exercises basic pandas, Numpy, and the graph ranking and analysis content of Notebook 11. It consists of five (5) exercises, numbered 0 through 4, worth a total of **11 points.**\n",
    "\n",
    "- Exercise 0: 3 points\n",
    "- Exercise 1: 2 points\n",
    "- Exercise 2: 1 point\n",
    "- Exercise 3: 3 points\n",
    "- Exercise 4: 2 points\n",
    "\n",
    "**All exercises are independent, so if you get stuck on one, try moving on to the next one.** However, in such cases do look for notes labeled, _\"In case Exercise XXX isn't working\"_, as you may need to run some code cells that load pre-computed results that will allow you to continue with any subsequent exercises.\n",
    "\n",
    "**Pro-tips.**\n",
    "- If your program behavior seem strange, try resetting the kernel and rerunning everything.\n",
    "- If you mess up this notebook or just want to start from scratch, save copies of all your partial responses and use `Actions` $\\rightarrow$ `Reset Assignment` to get a fresh, original copy of this notebook. (_Resetting will wipe out any answers you've written so far, so be sure to stash those somewhere safe if you intend to keep or reuse them!_)\n",
    "- If you generate excessive output (e.g., from an ill-placed `print` statement) that causes the notebook to load slowly or not at all, use `Actions` $\\rightarrow$ `Clear Notebook Output` to get a clean copy. The clean copy will retain your code but remove any generated output. **However**, it will also **rename** the notebook to `clean.xxx.ipynb`. Since the autograder expects a notebook file with the original name, you'll need to rename the clean notebook accordingly.\n",
    "\n",
    "**Good luck!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "D5Ftq53hzSl8",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Background: Transportation networks and infectious disease\n",
    "\n",
    "One major factor in the spread of infectious diseases like [COVID-19](https://www.cdc.gov/coronavirus/2019-nCoV/index.html) is the connectivity of our transportation networks. Therefore, let's ask the following question in this problem: to what extent does the connectivity of the airport network help explain in which regions we have seen the most confirmed cases of COVID-19?\n",
    "\n",
    "We'll focus on the United States network (recall Notebook 11) and analyze data at the level of US states (e.g., Washington state, California, New York state). Our analysis will have three main steps.\n",
    "\n",
    "1. Let's start by inspecting some recent COVID-19 data on the number of confirmed cases over time, to see which states are seeing the most cases.\n",
    "2. Next, let's (re-)analyze the airport network to rank the states by their likelihood of seeing air traffic.\n",
    "3. Finally, we'll compare the state ranking by incidence of COVID-19 with those by airport traffic, to see if there is any \"correlation\" between the two. We don't expect perfect overlap in these rankings, but if there is substantial overlap, it would provide evidence for the role that air transportation networks play in the spread of the disease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "KoXgQXzjzSmA",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Before starting, run the code cell below to load some useful functions and packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 414
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 1179,
     "status": "error",
     "timestamp": 1616733145232,
     "user": {
      "displayName": "Galen Hew",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjrtDAi0j1_uCFB3UcstLMQT1P_mHoLyzWqKUoW6cc=s64",
      "userId": "14593423718388120675"
     },
     "user_tz": -480
    },
    "id": "UPexpinlzSmC",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "outputId": "31cc841a-b8fb-4bd3-d9ae-1bcf8122e230"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.5 (default, Jan 27 2021, 15:41:15) \n",
      "[GCC 9.3.0]\n",
      "Pandas version: 1.2.3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "\n",
    "# Needed for loading data:\n",
    "import pandas as pd\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "\n",
    "# Some problem-specific helper functions:\n",
    "import problem_utils\n",
    "from problem_utils import get_path, assert_tibbles_are_equivalent\n",
    "\n",
    "# For visualization:\n",
    "from matplotlib.pyplot import figure, plot, semilogy, grid, legend\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "YBo2gor6zSmE",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Step 1: Inspecting COVID-19 incidence data by state\n",
    "\n",
    "Researchers at Johns Hopkins University have been tallying the number of confirmed cases of COVID-19 over time. Let's start by assembling the raw data for analysis.\n",
    "\n",
    "> *Provenance of these data.* JHU made these data are available in [this repo on GitHub](https://github.com/CSSEGISandData/COVID-19), but for this problem, we'll use a pre-downloaded copy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "uJ7E0ARdzSmG",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**Location of the data.** The data are stored in files, one for each day since January 22, 2020. We can use pandas's [`read_csv()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) to load them into a `DataFrame` object. For example, here is some code to do that for January 22, March 11, and March 22. Take a moment to read this code and observe the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rIe-AqukzSmI",
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location of data files: ./resource/asnlib/publicdata/covid19/\n",
      "Location of Jan 22 data: ./resource/asnlib/publicdata/covid19/01-22-2020.csv\n",
      "Loading...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './resource/asnlib/publicdata/covid19/01-22-2020.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a6593b8f09f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Location of Jan 22 data:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'covid19/01-22-2020.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'covid19/01-22-2020.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done loading. The first 5 rows:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/data_wrangler_py/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/data_wrangler_py/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/data_wrangler_py/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/data_wrangler_py/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/data_wrangler_py/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/data_wrangler_py/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \"\"\"\n\u001b[0;32m-> 1362\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/data_wrangler_py/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './resource/asnlib/publicdata/covid19/01-22-2020.csv'"
     ]
    }
   ],
   "source": [
    "print(\"Location of data files:\", get_path('covid19/'))\n",
    "print(\"Location of Jan 22 data:\", get_path('covid19/01-22-2020.csv'))\n",
    "print(\"Loading...\")\n",
    "df0 = pd.read_csv(get_path('covid19/01-22-2020.csv'))\n",
    "print(\"Done loading. The first 5 rows:\")\n",
    "df0.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wnY5c48CzSmK",
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(get_path('covid19/03-11-2020.csv'))\n",
    "df1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wL8qEu_wzSmL"
   },
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(get_path('covid19/03-22-2020.csv'))\n",
    "df2.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "gj9wRJWxzSmN",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**Columns.** Observe that the column conventions are changing over time, which will make working with this data quite messy if we don't deal with it.\n",
    "\n",
    "In this problem, we will only be interested in the following four columns:\n",
    "\n",
    "- `\"Province/State\"` or `\"Province_State\"`. If your code encounters the latter (`\"Province_State\"`), rename it to the former (`\"Province/State\"`).\n",
    "- `\"Country/Region\"` or `\"Country_Region\"`. Again, rename instances of the latter to the former.\n",
    "- `\"Last Update\"` or `\"Last_Update\"`. Again, rename the latter to the former.\n",
    "- `\"Confirmed\"`. This column is named consistently for all the example data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "tAy1ko8lzSmP",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**Missing values.** Observe that there may be missing values, which `read_csv()` converts by default to \"not-a-number\" (`NaN`) values. Recall that these are special floating-point values. As a by-product of using `NaN` values in columns that otherwise contain integers, those integers are *also* converted to floating-point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "BrNXesJgzSmQ",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**Timestamps.** Observe that each dataframe has a column named `\"Last Update\"`, which contain date and time values stored as _strings_. Moreover, they appear to use different formats. Later, we'll want to standardize these, and for that purpose, we'll use pandas's [`to_datetime()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html) to convert these into Python [`datetime`](https://docs.python.org/3/library/datetime.html) objects. That makes them easier to compare (in code) and do simple arithmetic on them (e.g., calculate the number of days in-between). The following code cells demonstrate these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S2oQ5tPJzSmU",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(type(df1['Last Update'].loc[0])) # Confirm that these values are strings\n",
    "\n",
    "# Example: Convert a column to use `datetime` values:\n",
    "df0['Timestamp'] = pd.to_datetime(df0['Last Update'])\n",
    "df1['Timestamp'] = pd.to_datetime(df1['Last Update'])\n",
    "df1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mG0-mlyZzSmV"
   },
   "outputs": [],
   "source": [
    "# Example: Calcuate the difference, in days, between two timestamps\n",
    "timestamp_0 = df1['Timestamp'].iloc[0]\n",
    "timestamp_1 = df1['Timestamp'].iloc[1]\n",
    "delta_t = timestamp_1 - timestamp_0\n",
    "\n",
    "print(f\"* {timestamp_0}  ==> type: {type(timestamp_0)}\")\n",
    "print(f\"* {timestamp_1}  ==> type: {type(timestamp_1)}\")\n",
    "print(f\"* Difference: ({timestamp_1}) - ({timestamp_0}) == {delta_t}\\n  ==> type: {type(delta_t)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "J5XWD6wVzSmX",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "You won't need to do date-time arithmetic directly, but standardizing in this way will facilitate things like sorting the data by timestamp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "pe7oqunKzSmY",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**Getting a list of available data files.** Lastly, here is a function to get a list of available daily data files by filename. You don't need to read this code, but do observe the results of the demo call to see how it is useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "tbpUmrBuzSma",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def get_covid19_daily_filenames(root=get_path(\"covid19/\")):\n",
    "    \"\"\"\n",
    "    Returns a list of file paths corresponding to JHU's\n",
    "    daily tallies of COVID-19 cases.\n",
    "    \"\"\"\n",
    "    from os import listdir\n",
    "    from os.path import isfile\n",
    "    from re import match\n",
    "    \n",
    "    def covid19_filepath(filebase, root):\n",
    "        return f\"{root}{filebase}\"\n",
    "    \n",
    "    def is_covid19_daily_file(filebase, root):\n",
    "        file_path = covid19_filepath(filebase, root)\n",
    "        return isfile(file_path) and match('^\\d\\d-\\d\\d-2020.csv$', filebase)\n",
    "    \n",
    "    filenames = []\n",
    "    for b in listdir(root):\n",
    "        if is_covid19_daily_file(b, root):\n",
    "            filenames.append(covid19_filepath(b, root))\n",
    "    return sorted(filenames)\n",
    "\n",
    "# Demo:\n",
    "print(repr(get_covid19_daily_filenames()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "tVL0w7wazSmc",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Exercise 0 (3 points): Data loading and cleaning ###\n",
    "\n",
    "Given `filenames`, a list of filenames that might be generated by `get_covid19_filenames()` above, complete the function `load_covid19_daily_data(filenames)` below so that it reads all of this data and combines it into a single tibble (as a pandas `DataFrame`) containing only the following columns:\n",
    "\n",
    "* `\"Province/State\"`: Same contents as the original data frames.\n",
    "* `\"Country/Region\"`: Same contents as the original data frames.\n",
    "* `\"Confirmed\"`: Same contents as the original data frames.\n",
    "* `\"Timestamp\"`: The values from the `\"Last Update\"` columns, but **converted** to `datetime` objects per the demonstration discussed previously.\n",
    "\n",
    "In addition, your code should do the following:\n",
    "\n",
    "1. Don't forget that sometimes `\"Province/State\"`, `\"Country/Region\"`, and `\"Last Update\"` are written differently, so be sure to handle those cases.\n",
    "1. If there are any duplicate rows (i.e., two or more rows whose values are identical), only one of the rows should be retained.\n",
    "1. In the `\"Confirmed\"` column, any missing values should be replaced by (0). Also, this column should be converted to have an integer type. (_Hint:_ Consider `Series.fillna()` and `Series.astype()`.)\n",
    "1. Your code should *not* depend on the input files having any specific columns other than the ones directly relevant to producing the above output, i.e., `\"Province/State\"`, `\"Country/Region\"`, `\"Confirmed\"`, and `\"Last Update\"`. It should also not depend on any particular ordering of the columns.\n",
    "\n",
    "> **Hint 0.** Per the preceding examples, use `pd.read_csv()` to read the contents of each file into a data frame. However, the `filenames` list will already include a valid path, so you do **not** need to use `get_path()`.\n",
    ">\n",
    "> **Hint 1.** Recall that you can use `pd.concat()` to concatenate data frames; one tweak in here is to use its `ignore_index=True` parameter to get a clean tibble-like index.\n",
    ">\n",
    "> **Hint 2.** To easily drop duplicate rows, look for a relevant pandas built-in function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "apifWwzlzSme"
   },
   "outputs": [],
   "source": [
    "def load_covid19_daily_data(filenames):\n",
    "    ###\n",
    "    ### YOUR CODE HERE\n",
    "    ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tJUiPY-SzSmg"
   },
   "outputs": [],
   "source": [
    "# Demo of your function:\n",
    "df = load_covid19_daily_data(get_covid19_daily_filenames())\n",
    "\n",
    "print(f\"There are {len(df)} rows in your data frame.\")\n",
    "print(\"The first five are:\")\n",
    "display(df.head(5))\n",
    "\n",
    "print(\"A random sample of five additional rows:\")\n",
    "df.sample(5).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "OkefrGvVzSmi",
    "nbgrader": {
     "grade": true,
     "grade_id": "ex0__load_covid19_daily_data",
     "locked": true,
     "points": "3",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Test cell: `ex0__load_covid19_daily_data` (3 points)\n",
    "\n",
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n",
    "\n",
    "def ex0_random_value():\n",
    "    from random import random, randint, choice\n",
    "    from numpy import nan\n",
    "    from problem_utils import ex0_random_date, ex0_random_string\n",
    "    options = [randint(-100, 100) # int\n",
    "               , ex0_random_string(randint(1, 10)) # string\n",
    "               , ex0_random_date(), # date\n",
    "               '', # implicit NaN\n",
    "               nan # explicit NaN\n",
    "              ]\n",
    "    return choice(options)\n",
    "\n",
    "def ex0_get_locales(filename=get_path(\"locales.json\")):\n",
    "    from json import load\n",
    "    with open(filename, \"rt\") as fp:\n",
    "        locales = load(fp)\n",
    "    return locales\n",
    "\n",
    "def ex0_gen_row(locales, num_dummies=0):\n",
    "    from datetime import datetime\n",
    "    from random import choice, random, randint\n",
    "    from numpy import nan\n",
    "    from problem_utils import ex0_random_date\n",
    "    country = choice(list(locales.keys()))\n",
    "    province = nan if random() <= 0.1 else choice(locales[country])\n",
    "    confirmed = 0 if random() <= 0.1 else randint(1, 100000)\n",
    "    last_updated = ex0_random_date()\n",
    "    if num_dummies:\n",
    "        dummy_vals = tuple([ex0_random_value() for _ in range(num_dummies)])\n",
    "    else:\n",
    "        dummy_vals = ()\n",
    "    return (country, province, confirmed, last_updated, *dummy_vals)\n",
    "    \n",
    "def ex0_gen_df():\n",
    "    from random import randint, random\n",
    "    from pandas import DataFrame\n",
    "    from problem_utils import ex0_random_string\n",
    "    \n",
    "    locales = ex0_get_locales()\n",
    "\n",
    "    # Generate random columns, which the student should ignore\n",
    "    num_dummy_cols = randint(1, 4)\n",
    "    dummy_cols = []\n",
    "    while len(dummy_cols) != num_dummy_cols:\n",
    "        dummy_cols = list({ex0_random_string(5) for _ in range(num_dummy_cols)})\n",
    "    \n",
    "    # Generate a bunch of random rows\n",
    "    num_trials = randint(10, 50)\n",
    "    rows = [ex0_gen_row(locales, num_dummy_cols) for _ in range(num_trials)]\n",
    "    \n",
    "    # Remove any initial duplicates\n",
    "    rows = sorted(rows, key=lambda x: repr(x))\n",
    "    rows_soln = [rows[0]]\n",
    "    for r in rows[1:]:\n",
    "        if repr(r) != repr(rows_soln[-1]):\n",
    "            rows_soln.append(r)\n",
    "\n",
    "    # Construct the solution tibble\n",
    "    cols_in = [\"Country/Region\" if random() < 0.75 else \"Country_Region\",\n",
    "               \"Province/State\" if random() < 0.75 else \"Province_State\",\n",
    "               \"Confirmed\",\n",
    "               \"Last Update\" if random() < 0.75 else \"Last_Update\"]\n",
    "    cols_out = [\"Country/Region\", \"Province/State\", \"Confirmed\", \"Last Update\"]\n",
    "    df_soln = DataFrame(rows_soln, columns=cols_out + dummy_cols)[cols_out] \\\n",
    "              .rename(columns={\"Last Update\": \"Timestamp\"})\n",
    "    \n",
    "    # Generate a corresponding input tibble\n",
    "    rows_in = []\n",
    "    for r in rows_soln:\n",
    "        s = list(r)\n",
    "        if s[2] == 0:\n",
    "            s[2] = '' # NaN counts\n",
    "        r_in = tuple(s)\n",
    "        rows_in.append(r_in)\n",
    "        if random() <= 0.15: # Random duplicates\n",
    "            for _ in range(randint(1, 4)):\n",
    "                rows_in.append(r_in)\n",
    "    df_in = DataFrame(rows_in, columns=cols_in + dummy_cols)\n",
    "    \n",
    "    return df_in, df_soln\n",
    "\n",
    "def ex0_split_df(df, max_splits=5):\n",
    "    from random import randint\n",
    "    from numpy import arange, sort, append\n",
    "    from numpy.random import shuffle, choice\n",
    "    # Shuffle the rows\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # Split the rows\n",
    "    df_split = []\n",
    "    num_splits = min(randint(0, max_splits), len(df))\n",
    "    if num_splits > 0:\n",
    "        split_inds = sort(choice(arange(len(df)), size=num_splits, replace=False))\n",
    "        if split_inds[0] > 0:\n",
    "            split_inds = append(0, split_inds)\n",
    "        if split_inds[-1] < len(df):\n",
    "            split_inds = append(split_inds, len(df))\n",
    "        for i, j in zip(split_inds[:-1], split_inds[1:]):\n",
    "            df_ij = df.iloc[i:j].reset_index(drop=True)  \n",
    "            df_split.append(df_ij)\n",
    "        \n",
    "    return df_split if num_splits else [df]\n",
    "\n",
    "def ex0_certify_metadata(df):\n",
    "    df_cols = set(df.columns)\n",
    "    true_cols = {\"Province/State\", \"Country/Region\", \"Confirmed\", \"Timestamp\"}\n",
    "    too_many_cols = df_cols - true_cols\n",
    "    assert not too_many_cols, f\"*** You have too many columns, including {too_many_cols}. ***\"\n",
    "    missing_cols = true_cols - df_cols\n",
    "    assert not missing_cols, f\"*** You are missing some columns, namely, {missing_cols}. ***\"\n",
    "\n",
    "    from pandas.api.types import is_integer_dtype\n",
    "    assert is_integer_dtype(df[\"Confirmed\"]), \\\n",
    "           '*** `\"Confirmed\"` column has a non-integer type ({type(df[\"Confirmed\"])}). ***'\n",
    "    \n",
    "    from numpy import datetime64\n",
    "    from pandas import Index\n",
    "    assert df.select_dtypes(include=datetime64).columns == Index([\"Timestamp\"]), \\\n",
    "           '*** Your data frame must have a \"Timestamp\" column containing `datetime` values.'\n",
    "    \n",
    "def ex0_check():\n",
    "    from problem_utils import canonicalize_tibble, tibbles_left_matches_right\n",
    "    from os import remove\n",
    "    from os.path import isfile\n",
    "    print(\"Generating synthetic input files...\")\n",
    "    df_in, df_soln = ex0_gen_df()\n",
    "    df_split = ex0_split_df(df_in)\n",
    "    filenames = []\n",
    "    for k, df_k in enumerate(df_split):\n",
    "        filenames.append(f'./ex0_df{k}.csv')\n",
    "        print(f\"- {filenames[-1]}\")\n",
    "        df_k.to_csv(filenames[-1], index=False)\n",
    "    try:\n",
    "        print(\"Testing your solution...\")\n",
    "        df = load_covid19_daily_data(filenames)\n",
    "        ex0_certify_metadata(df)\n",
    "        assert tibbles_left_matches_right(df, df_soln, verbose=True), \\\n",
    "               \"*** Your computed solution does not match ours. ***\"\n",
    "    except:\n",
    "        print(\"\\n=== Expected solution ===\")\n",
    "        display(canonicalize_tibble(df_soln, remove_index=True))\n",
    "        print(\"\\n=== Your computed solution ===\")\n",
    "        display(canonicalize_tibble(df, remove_index=True))\n",
    "        print(f\"\\nNOTE: To see the original input files, inspect {filenames}.\")\n",
    "        raise\n",
    "    else:\n",
    "        print(\"Cleaning up input files...\")\n",
    "        for f in filenames:\n",
    "            if isfile(f):\n",
    "                print(f\"- {f}\")\n",
    "                remove(f)\n",
    "        \n",
    "        \n",
    "for trial in range(5):\n",
    "    print(f\"========== Trial #{trial} ==========\")\n",
    "    ex0_check()\n",
    "\n",
    "print(\"\\n(Passed.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "oD_FJg3fzSmm",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### A combined data frame ###\n",
    "\n",
    "Whether you solved Exercise 0 or not, we have prepared a file of pre-cleaned and combined COVID-19 data. Below, the variable `df_covid19` holds these data. You will need it in the subsequent exercises, so be sure to run this cell and do not modify the variable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ATRDNyh3zSmn",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "df_covid19 = pd.read_csv(get_path('covid19/ex0_soln.csv'), parse_dates=[\"Timestamp\"])\n",
    "df_covid19 = df_covid19.groupby([\"Province/State\", \"Country/Region\", \"Timestamp\"], as_index=False).sum()\n",
    "             # ^^^ Above `.groupby()` needed because of a change in US reporting on March 22, 2020\n",
    "df_covid19.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "QYpSzMr8zSmn",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### US state-by-state data ###\n",
    "\n",
    "The dataset includes confirmed cases in the US. For instance, run this cell to see a sample of the US rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bw_lcwVtzSmo"
   },
   "outputs": [],
   "source": [
    "is_us = (df_covid19[\"Country/Region\"] == \"US\")\n",
    "df_covid19[is_us].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "DWTiy_x8zSmp",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "You should see some cases where the `\"Province/State\"` field is exactly the name of a US state, like `\"Georgia\"` or `\"California\"`, and other cases where you might see a more or less detailed location (e.g., a city name and a statee, like `\"Middlesex County, MA\"`).\n",
    "\n",
    "For subsequent analysis, we will only be interested in the rows containing **state names**. For instance, here are all the rows associated with `\"Georgia\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fOjCjxoMzSmq"
   },
   "outputs": [],
   "source": [
    "is_georgia = (df_covid19[\"Province/State\"] == \"Georgia\")\n",
    "df_covid19[is_us & is_georgia]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "47W1xNoCzSmr",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Given these data, we can order by timestamp and plot confirmed cases over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HDQ8Kh7nzSmr"
   },
   "outputs": [],
   "source": [
    "df_covid19[is_us & is_georgia] \\\n",
    "    .sort_values(by=\"Timestamp\") \\\n",
    "    .plot(x=\"Timestamp\", y=\"Confirmed\", figsize=(16, 9), style='*--')\n",
    "grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "xjiRbEvvzSms",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Exercise 1 (2 points): US state-by-state data\n",
    "\n",
    "Complete the function, `get_us_states(df)`, below, where\n",
    "\n",
    "- its input, `df`, is a data frame structured like the combined COVID-19 data frame (`df_covid19`), having the columns `\"Province/State\"`, `\"Country/Region\"`, `\"Confirmed\"`, `\"Timestamp\"`;\n",
    "- and it returns a tibble containing only those rows of `df` that are from the United States where the `\"Province/State\"` field is exactly the name of any one of the US states.\n",
    "\n",
    "Regarding the second requirement, the returned object should include a row where the `\"Province/State\"` field is `\"Georgia\"`, but it should **not** include a row where this field is, say, `\"Atlanta, GA\"`. (Put differently, we will assume the state-level accounts already include city-level counts.)\n",
    "\n",
    "The tibble returned by your function should only have these three columns:\n",
    "\n",
    "1. `\"Confirmed\"`: The number of confirmed cases, taken from the input `df`.\n",
    "2. `\"Timestamp\"`: The timestamp taken from the input `df`.\n",
    "3. `\"ST\"`: The two-letter **abbreviation** for the state's name.\n",
    "\n",
    "Pay attention to item (3): your returned tibble should not have the state's full name, but rather, its two-letter postal code abbreviation (e.g., `\"GA\"` instead of `\"Georgia\"`). To help you out, here is a code cell that defines a data frame called `STATE_NAMES` that holds both a list of state names and their two-letter abbreviations.\n",
    "\n",
    "> **Note**: The test cell for this exercise reuses functions defined in the test cell for Exercise 0. So even if you skipped Exercise 0, please run its test cell before running the one below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "AyvN0nVmzSmt",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "STATE_NAMES = pd.read_csv(get_path('us_states.csv'))\n",
    "print(f\"There are {len(STATE_NAMES)} US states. The first and last three, along with their two-letter postal code abbreviations, are as follows (in alphabetical order):\")\n",
    "display(STATE_NAMES.head(3))\n",
    "print(\"...\")\n",
    "display(STATE_NAMES.tail(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "muDpbu9ozSmu"
   },
   "outputs": [],
   "source": [
    "def get_us_states(df):\n",
    "    ###\n",
    "    ### YOUR CODE HERE\n",
    "    ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "9TIZyGAhzSmv",
    "nbgrader": {
     "grade": true,
     "grade_id": "ex1__get_us_states",
     "locked": true,
     "points": "2",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Test cell: `ex1__get_us_states` (2 points)\n",
    "\n",
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n",
    "\n",
    "def ex1_gen_row(states):\n",
    "    from datetime import datetime\n",
    "    from random import random, randint, choice\n",
    "    from problem_utils import ex0_random_date, ex0_random_string\n",
    "    def rand_str(): return ex0_random_string(randint(1, 10))\n",
    "    \n",
    "    confirmed = randint(1, 10000)\n",
    "    timestamp = ex0_random_date()\n",
    "    \n",
    "    # Choose a province\n",
    "    locales = ex0_get_locales()\n",
    "    p = random()\n",
    "    if p < 0.5: # Non US country\n",
    "        country = choice(list(set(locales.keys()) - {\"US\"}))\n",
    "        province = choice(locales[country])\n",
    "        is_state = False\n",
    "    else:\n",
    "        country = \"US\"\n",
    "        if p < 0.75:\n",
    "            non_states = set(locales[\"US\"]) - set(states[\"Name\"])\n",
    "            province = choice(list(non_states))\n",
    "            is_state = False\n",
    "        else:\n",
    "            province = choice(states[\"Name\"])\n",
    "            is_state = True\n",
    "    return timestamp, confirmed, country, province, is_state\n",
    "\n",
    "def ex1_gen_df(max_rows, states):\n",
    "    from random import randint\n",
    "    from pandas import DataFrame, concat\n",
    "    st_lookup = states.set_index(\"Name\")\n",
    "    num_rows = randint(1, max_rows)\n",
    "    df_list = []\n",
    "    cols_df = [\"Timestamp\", \"Confirmed\", \"Country/Region\", \"Province/State\"]\n",
    "    df_soln_list = []\n",
    "    cols_df_soln = [\"Timestamp\", \"Confirmed\", \"ST\"]\n",
    "    for _ in range(num_rows):\n",
    "        ts, conf, country, province, is_state = ex1_gen_row(states)\n",
    "        df0 = DataFrame([[ts, conf, country, province]], columns=cols_df)\n",
    "        df_list.append(df0)\n",
    "        if is_state:\n",
    "            st = st_lookup.loc[province][\"Abbrv\"]\n",
    "            df0_soln = DataFrame([[ts, conf, st]], columns=cols_df_soln)\n",
    "            df_soln_list.append(df0_soln)\n",
    "    assert len(df_list) > 0, \"*** Problem with the test cell! ***\"\n",
    "    df = concat(df_list, ignore_index=True).sample(frac=1).reset_index(drop=True)\n",
    "    if len(df_soln_list) == 0:\n",
    "        df_soln = DataFrame(columns=cols_df_soln)\n",
    "    else:\n",
    "        df_soln = concat(df_soln_list, ignore_index=True).sample(frac=1).reset_index(drop=True)\n",
    "    return df, df_soln\n",
    "\n",
    "def ex1_check():\n",
    "    df, df_soln = ex1_gen_df(20, STATE_NAMES)\n",
    "    try:\n",
    "        df_your_soln = get_us_states(df)\n",
    "        assert_tibbles_are_equivalent(df_soln, df_your_soln)\n",
    "    except:\n",
    "        print(\"\\n*** ERROR DETECTED ***\")\n",
    "        print(\"Input data frame:\")\n",
    "        display(df)\n",
    "        print(\"Expected solution:\")\n",
    "        display(df_soln)\n",
    "        print(\"Your solution:\")\n",
    "        display(df_your_soln)\n",
    "        raise\n",
    "        \n",
    "for trial in range(10):\n",
    "    print(f\"=== Trial #{trial} / 9 ===\")\n",
    "    ex1_check()\n",
    "\n",
    "print(\"\\n(Passed.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "2lrwAjPlzSmw",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### US state-by-state data ###\n",
    "\n",
    "Whether your Exercise 1 is working or not, please run the following code cell. It loads a pre-generated data frame containing just the state-level COVID-19 confirmed cases data into a variable named, `df_covid19_us`. You will need it in the subsequent exercises, so do not modify it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "258CvnuVzSmw",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "df_covid19_us = pd.read_csv(get_path('covid19/ex1_soln.csv'), parse_dates=[\"Timestamp\"])\n",
    "df_covid19_us.sample(5).sort_values(by=[\"ST\", \"Timestamp\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "CKfxTjV3zSmx",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Exercise 2 (1 point): Ranking by confirmed cases\n",
    "\n",
    "Let `df` be a data frame like `df_covid19_us`, which would be produced by a correctly functioning `get_us_states()` (Exercise 1). Complete the function `rank_states_by_cases(df)` so that it returns a **Python `list`** of states in decreasing order of the **maximum** number of confirmed cases in that state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mJuad6awzSmy"
   },
   "outputs": [],
   "source": [
    "def rank_states_by_cases(df):\n",
    "    ###\n",
    "    ### YOUR CODE HERE\n",
    "    ###\n",
    "    \n",
    "your_covid19_rankings = rank_states_by_cases(df_covid19_us)\n",
    "assert isinstance(your_covid19_rankings, list), \"Did you return a Python `list` as instructed?\"\n",
    "print(f\"Your computed ranking:\\n==> {repr(your_covid19_rankings)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dUQLl4BRzSmz"
   },
   "outputs": [],
   "source": [
    "df_covid19_us.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "cUSkkNC9zSmz",
    "nbgrader": {
     "grade": true,
     "grade_id": "ex2__rank_states_by_cases",
     "locked": true,
     "points": "1",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Test cell: `ex2__rank_states_by_cases` (1 point)\n",
    "\n",
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n",
    "\n",
    "def ex2_gen_df(st):\n",
    "    from problem_utils import ex0_random_date\n",
    "    from random import randint\n",
    "    from pandas import DataFrame\n",
    "    num_rows = randint(1, 5)\n",
    "    confs = []\n",
    "    tss = []\n",
    "    max_conf = -1\n",
    "    for k in range(num_rows):\n",
    "        confs.append(randint(1, 1000))\n",
    "        if confs[-1] > max_conf: max_conf = confs[-1]\n",
    "        tss.append(ex0_random_date())\n",
    "    df_st = DataFrame({\"ST\": [st] * num_rows, \"Confirmed\": confs, \"Timestamp\": tss})\n",
    "    return df_st, max_conf\n",
    "\n",
    "def ex2_check():\n",
    "    from random import randint, sample\n",
    "    from pandas import concat\n",
    "    num_states = randint(1, 5)\n",
    "    states = sample(list(STATE_NAMES[\"Abbrv\"]), num_states)\n",
    "    vals = []\n",
    "    df_list = []\n",
    "    for st in states:\n",
    "        df_st, max_conf = ex2_gen_df(st)\n",
    "        df_list.append(df_st)\n",
    "        vals.append((st, max_conf))\n",
    "    df = concat(df_list, ignore_index=True).sort_values(by=\"Timestamp\").reset_index(drop=True)\n",
    "    soln = [s for s, v in sorted(vals, key=lambda x: x[1], reverse=True)]\n",
    "    try:\n",
    "        your_soln = rank_states_by_cases(df)\n",
    "        assert len(soln) == len(your_soln), \\\n",
    "               f\"*** Your solution has {len(your_soln)} entries instead of {len(soln)} ***\"\n",
    "        assert all([a == b for a, b in zip(soln, your_soln)]), \\\n",
    "               f\"*** Solutions do not match ***\"\n",
    "    except:\n",
    "        print(\"\\n*** ERROR CASE ***\\n\")\n",
    "        print(\"Input:\")\n",
    "        display(df)\n",
    "        print(\"Expected solution:\")\n",
    "        display(soln)\n",
    "        print(\"Your solution:\")\n",
    "        display(your_soln)\n",
    "        raise\n",
    "        \n",
    "for trial in range(10):\n",
    "    print(f\"=== Trial #{trial} / 9 ===\")\n",
    "    ex2_check()\n",
    "\n",
    "print(\"\\n(Passed.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "RtkMHmlmzSm0",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### (In case Exercise 2 isn't working) Ranking by confirmed cases ###\n",
    "\n",
    "In case you can't get a working solution to Exercise 2, we have prepared a ranked list of states by confirmed cases. The code cell below reads this list and stores it in the variable, `covid19_rankings`. You will need it in the subsequent exercises, so do not modify it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "JS-vNTWkzSm1",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "with open(get_path('covid19/ex2_soln.txt'), \"rt\") as fp:\n",
    "    covid19_rankings = [s.strip() for s in fp.readlines()]\n",
    "print(repr(covid19_rankings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "WPyJt1UozSm2",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Visualization ###\n",
    "\n",
    "Let's plot the `TOP_K=15` states by number of confirmed cases. **The y-axis uses a logarithmic scale in this plot.**\n",
    "\n",
    "> To disable a logarithmic y-axis, add `logy=False` to any call to `viz_by_state()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "id": "DgzrZwvbzSm2",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def viz_by_state(col, df, states, figsize=(16, 9), logy=False):\n",
    "    from matplotlib.pyplot import figure, plot, semilogy, legend, grid\n",
    "    figure(figsize=figsize)\n",
    "    plotter = plot if not logy else semilogy\n",
    "    for s in states:\n",
    "        df0 = df[df[\"ST\"] == s].sort_values(by=\"Timestamp\")\n",
    "        plotter(df0[\"Timestamp\"], df0[col], \"o:\")\n",
    "    legend(states)\n",
    "    grid()\n",
    "    \n",
    "TOP_K = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NFKgx7wuzSm3"
   },
   "outputs": [],
   "source": [
    "# You can modify this cell if you want to play around with the visualization.\n",
    "\n",
    "viz_by_state(\"Confirmed\", df_covid19_us, covid19_rankings[:TOP_K], logy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "j5uIySHxzSm4",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Observe that this data is irregularly sampled and noisy. For instance, the updates do not occur every day in every state, and there are spikes due to reporting errors. Therefore, it would be useful to \"smooth out\" the data before plotting it, to help discern the overall trends better. That is your next task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "m60mr4WBzSm5",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Filling-in missing values ###\n",
    "\n",
    "We'll do a first cleaning step for you: filling-in (or _imputing_) missing daily values, so that we have at least one value per day. To see the issue more clearly, consider the data for the state of Georgia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_OrTliOTzSm6"
   },
   "outputs": [],
   "source": [
    "df_covid19_us[df_covid19_us[\"ST\"] == \"GA\"].sort_values(by=\"Timestamp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "T_J9zelkzSm6",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "There are two observations on March 11 and no observations on March 13. Suppose we want one value per day for every state. Our approach will be to _resample_ the values, using pandas built-in [resampler](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#resampling), a standard cleaning method when dealing with irregularly sampled time-series data. There are many subtle options, so we will perform one method of resampling for you. The function below implements it, storing the results in a data frame called `df_us_daily`. You do not need to understand this code right, but do run it so you can see what it will do. It will print some example results for the state of Georgia data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "zFOE4FObzSm7",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def resample_daily(df):\n",
    "    # This implementation is a bit weird, due to a known issue: https://github.com/pandas-dev/pandas/issues/28313\n",
    "    df_r = df.sort_values(by=[\"ST\", \"Timestamp\"]) \\\n",
    "             .set_index(\"Timestamp\") \\\n",
    "             .groupby(\"ST\", group_keys=False) \\\n",
    "             .resample(\"1D\", closed=\"right\") \\\n",
    "             .ffill() \\\n",
    "             .reset_index()\n",
    "    return df_r.sort_values(by=[\"ST\", \"Timestamp\"]).reset_index(drop=True)\n",
    "    \n",
    "df_us_daily = resample_daily(df_covid19_us)\n",
    "df_us_daily[df_us_daily[\"ST\"] == \"GA\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "4H7gn1L3zSm8",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Observe how there are now samples on every consecutive day beginning on March 11."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "5RBV8FVxzSm9",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Windowed daily averages ###\n",
    "\n",
    "Armed with regularly sampled data, you can now complete the next step, which is to smooth out the data using _windowed daily averages_, defined as follows.\n",
    "\n",
    "Let $c_t$ denote the number of confirmed cases on day $t$, and let $d$ be a positive integer. Then the $d$-day windowed daily average on day $t$, denoted $\\bar{c}_t$, is the mean number of confirmed cases in the $d$ days up to and including day $t$. Mathematically,\n",
    "\n",
    "$$\n",
    "\\bar{c}_t = \\frac{c_{t-(d-1)} + c_{t-(d-2)} + \\cdots + c_{t-1} + c_t}{d}.\n",
    "$$\n",
    "\n",
    "We'll refer to the values in the numerator as the _window_ for day $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "llvod7SVzSm9",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "For example, suppose $c = [0, 0, 1, 2, 2, 3, 3, 4, 6, 10]$, where the first and last values are $c_0=0$ and $c_9=10$. Now suppose $d=3$ days. Then the windowed daily average on day $t$ is the average of confirmed cases on days $t-2$, $t-1$, and $t$:\n",
    "\n",
    "$$\n",
    "\\bar{c}_4 = \\frac{c_2 + c_3 + c_4}{3} = \\frac{1 + 2 + 2}{3} = \\frac{5}{3} = 1.666\\ldots.\n",
    "$$\n",
    "\n",
    "In this example, there aren't 3-days worth of observations for days 0 and 1. Let's treat these cases as undefined, meaning there is no average computable for those days. Therefore, the final result in this example would be\n",
    "\n",
    "$$\n",
    "\\bar{c} = [\\mbox{nan}, \\mbox{nan}, 0.333\\ldots, 1.0, 1.666\\ldots, 2.333\\ldots, 2.666\\ldots, 3.333\\ldots, 4.333\\ldots, 6.666\\ldots],\n",
    "$$\n",
    "\n",
    "where $\\mbox{nan}$ is a floating-point not-a-number value, which we will use a stand-in for an undefined average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "OMEueTH2zSm-",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Exercise 3 (3 points): Computing windowed daily averages ###\n",
    "\n",
    "Suppose you are given a data frame `df` like `df_us_daily`, which `resample_daily()` computed. That is, you may assume `df` has three columns named `\"Timestamp\"`, `\"ST\"`, and `\"Confirmed\"`. However, **daily observations may appear in any order within `df`.** (That is, **do not** assume they are grouped by state or sorted by timestamp _a priori_.)\n",
    "\n",
    "Please complete the function `daily_windowed_avg(df, days)` so that it calculates the windowed daily average using windows of size `days`. Your function should return a copy of `df` with a new column named `Avg` containing this average. For days with no defined average, your function should simply omit those days from the output.\n",
    "\n",
    "> **Note.** Although the example below shows data only for `\"GA\"`, the input `df` may have more than one state's worth of data in it. Therefore, your function will need to handle that case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "WvgJ5bcVzSm_",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "For example, suppose the rows in `df` with Georgia data are as follows:\n",
    "\n",
    "Timestamp|ST|Confirmed\n",
    "---|---|---\n",
    "2020-03-12|GA|42\n",
    "2020-03-17|GA|121\n",
    "2020-03-11|GA|17\n",
    "2020-03-15|GA|66\n",
    "2020-03-18|GA|146\n",
    "2020-03-16|GA|99\n",
    "2020-03-13|GA|31\n",
    "2020-03-14|GA|31\n",
    "\n",
    "Observe that the rows are not necessarily in timestamp order, so you'll need to deal with that. Among these rows, the first date is March 11 and the last is March 18.\n",
    "\n",
    "Now, suppose we use `days=3` and call your function on the full dataset (with all states), and then look at just the Georgia rows, we should see\n",
    "\n",
    "Timestamp|ST|Confirmed|Avg\n",
    "---|---|---|---\n",
    "2020-03-13|GA|31|30.000000\n",
    "2020-03-14|GA|31|34.6666...\n",
    "2020-03-15|GA|66|42.6666...\n",
    "2020-03-16|GA|99|65.3333...\n",
    "2020-03-17|GA|121|95.3333...\n",
    "2020-03-18|GA|146|122.0000...\n",
    "\n",
    "You can confirm that the first day of this result, March 13, 2020, is 30, which is the average of March 11-13 (17, 42, and 31 cases, respectively). The last day, March 18, is 122, the average of March 16-18 (99, 121, and 146 cases). March 11 and 12 do not appear because they do not have three days worth of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "iPlbOPcNzSnA",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "> **Note 0.** There are many approaches to this problem. If you have good mastery of pandas, you should be able to quickly assimilate and apply its [built-in `.rolling()` technique](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#window-and-resample-operations). Otherwise, it should also be straightforward to apply other techniques you already know.\n",
    ">\n",
    "> **Note 1.** To pass the autograder, you'll need to ensure that your data frame has exactly the columns shown in the above example. (We use tibble-equivalency checks so column and row ordering does not matter.)\n",
    ">\n",
    "> **Note 2.** The `.dtype` of columns `\"Timestamp\"`, `\"ST\"`, and `\"Confirmed\"` should match those of the input; the new column `\"Avg\"` contains floating-point values, and so should have a floating-point `.dtype`.\n",
    ">\n",
    "> **Note 3.** Our tester already does _approximate_ checking for floating-point values. Therefore, if the test code reports a mismatch, you are definitely miscalculating the averages by much more than the amount allowed by roundoff error, and you will have to keep debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "ToUTZ81ozSnA"
   },
   "outputs": [],
   "source": [
    "def daily_windowed_avg(df, days):\n",
    "    ###\n",
    "    ### YOUR CODE HERE\n",
    "    ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J3LaPzf5zSnB",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Demo of your function:\n",
    "print('=== Two states: \"AK\" and \"GA\" ===')\n",
    "is_ak_ga_before = df_us_daily[\"ST\"].isin([\"AK\", \"GA\"])\n",
    "display(df_us_daily[is_ak_ga_before])\n",
    "\n",
    "print('=== Your results (days=3) ===')\n",
    "df_us_daily_avg = daily_windowed_avg(df_us_daily, 3)\n",
    "is_ak_ga_after = df_us_daily_avg[\"ST\"].isin([\"AK\", \"GA\"])\n",
    "display(df_us_daily_avg[is_ak_ga_after])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "CdyV90ehzSnC",
    "nbgrader": {
     "grade": true,
     "grade_id": "ex3__daily_windowed_avg",
     "locked": true,
     "points": "3",
     "solution": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test cell: `ex3__daily_windowed_avg` (3 points)\n",
    "\n",
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n",
    "\n",
    "def ex3_gen_state_df(st, days):\n",
    "    from random import randint, random\n",
    "    from pandas import DataFrame, concat\n",
    "    from problem_utils import ex0_random_date\n",
    "    def rand_day():\n",
    "        from datetime import datetime\n",
    "        date = ex0_random_date()\n",
    "        return datetime(date.year, date.month, date.day)\n",
    "    def inc_date(date, days=1):\n",
    "        from datetime import timedelta\n",
    "        return date + timedelta(days=days)\n",
    "    dates = []\n",
    "    sts = []\n",
    "    confs = [randint(1, 10)]\n",
    "    avgs = []\n",
    "    r0 = 1 + random()\n",
    "    day = rand_day()\n",
    "    num_days = days + randint(1, 10)\n",
    "    for k in range(num_days):\n",
    "        dates.append(day)\n",
    "        sts.append(st)\n",
    "        confs.append(int(confs[0] * (r0**k)))\n",
    "        if k >= days-1: avgs.append(sum(confs[(-days):]) / days)\n",
    "        day = inc_date(day)\n",
    "    df = DataFrame({\"Timestamp\": dates,\n",
    "                    \"ST\": sts,\n",
    "                    \"Confirmed\": confs[1:]})\n",
    "    df_soln = DataFrame({\"Timestamp\": dates[(days-1):],\n",
    "                         \"ST\": sts[(days-1):],\n",
    "                         \"Confirmed\": confs[days:],\n",
    "                         \"Avg\": avgs})\n",
    "    return df, df_soln\n",
    "\n",
    "def ex3_gen_df():\n",
    "    from random import randint, sample\n",
    "    from pandas import concat\n",
    "    num_states = randint(1, 4)\n",
    "    days = randint(1, 4)\n",
    "    states = sample(STATE_NAMES[\"Abbrv\"].tolist(), num_states)\n",
    "    df_list = []\n",
    "    df_soln_list = []\n",
    "    for st in states:\n",
    "        df_st, df_st_soln = ex3_gen_state_df(st, days)\n",
    "        df_list.append(df_st)\n",
    "        df_soln_list.append(df_st_soln)\n",
    "    df = concat(df_list, ignore_index=True).sample(frac=1).reset_index(drop=True)\n",
    "    df_soln = concat(df_soln_list, ignore_index=True).sort_values(by=[\"ST\", \"Timestamp\"])\n",
    "    try:\n",
    "        df_your_soln = daily_windowed_avg(df, days)\n",
    "        assert_tibbles_are_equivalent(df_soln, df_your_soln)\n",
    "    except:\n",
    "        print(\"\\n*** ERROR ***\")\n",
    "        print(\"Input data frame:\")\n",
    "        display(df)\n",
    "        print(f\"Expected solution (days={days}):\")\n",
    "        display(df_soln)\n",
    "        print(\"Your solution:\")\n",
    "        display(df_your_soln)\n",
    "        raise\n",
    "\n",
    "for trial in range(10):\n",
    "    print(f\"=== Trial #{trial} / 9 ===\")\n",
    "    ex3_gen_df()\n",
    "\n",
    "print(\"\\n(Passed.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Soz469XhzSnD",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### (In case Exercise 3 isn't working) Daily windowed averages ###\n",
    "\n",
    "In case you can't get a working solution to Exercise 3, we have pre-computed the daily windowed averages. The code cell below reads this data and stores it in the variable, `df_us_daily_avg`. You will need it in the subsequent exercises, so do not modify it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "3RyJ6xIlzSnE",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "with open(get_path('covid19/ex3_soln.csv'), \"rt\") as fp:\n",
    "    df_us_daily_avg = pd.read_csv(get_path('covid19/ex3_soln.csv'), parse_dates=[\"Timestamp\"])\n",
    "df_us_daily_avg[df_us_daily_avg[\"ST\"].isin([\"AK\", \"GA\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "X1NgWjrMzSnE",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Here is a visualization of the daily averages, which should appear smoother. As such, the trends should be a little more clear as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LM8CjSS9zSnF"
   },
   "outputs": [],
   "source": [
    "# You can modify this cell if you want to play around with the visualization.\n",
    "\n",
    "viz_by_state(\"Avg\", df_us_daily_avg, covid19_rankings[:TOP_K], logy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "7-NJvV1-zSnG",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Step 2: Flights (re-)analysis\n",
    "\n",
    "Recall from Notebook 11 that you used a Markov chain-based model to \"rank\" airport networks by how likely a certain \"random flyer\" is to end up at each airport. In this final step of this problem, you'll apply a similiar idea to rank states, and see how well it correlates with the state-by-state numbers of confirmed COVID-19 cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "0IGte2y2zSnH",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**Raw data.** First, observe that our raw data differs slightly from Notebook 11. It consists of all flights from calendar year 2019 (the latest available from the original source, as no 2020 flights are present there), and we've added a column with each airport's two-letter state postal code. Let's load these flights into a DataFrame called `flights`. (You don't need to understand this code in depth, but do pay attention to the format of the output sample from `flights`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "j4SdiyrczSnH",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def load_flights(infile=get_path('us-flights/us-flights-2019--86633396_T_ONTIME_REPORTING.csv')):\n",
    "    keep_cols = [\"FL_DATE\", \"ORIGIN_STATE_ABR\", \"DEST_STATE_ABR\", \"OP_UNIQUE_CARRIER\", \"OP_CARRIER_FL_NUM\"]\n",
    "    flights = pd.read_csv(infile)[keep_cols]\n",
    "    us_sts = set(STATE_NAMES[\"Abbrv\"])\n",
    "    origin_is_state = flights['ORIGIN_STATE_ABR'].isin(us_sts)\n",
    "    dest_is_state = flights['DEST_STATE_ABR'].isin(us_sts)\n",
    "    return flights.loc[origin_is_state & dest_is_state].copy()\n",
    "\n",
    "flights = load_flights()\n",
    "print(f\"There are {len(flights):,} direct flight segments in the `flights` data frame.\")\n",
    "print(\"Here are the first few:\")\n",
    "flights.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "0q92bAJszSnI",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**Outdegrees.** In Notebook 11, we calculated the outdegree of each airport $u$ to be the number of distinct endpoints (other airports) reachable from $u$.\n",
    "\n",
    "For the analysis in this problem, we will use a _different_ definition for the outdegree. In particular, we'll define the outdegree $d_u$ of **state** $u$ (e.g., the state of Georgia, the state of California) to be the total number of direct flight segments from state $u$ to all other states. In pandas, we can use a group-by-count aggregation to compute these outdegrees. Here is some code that does so, producing a data frame named `outdegrees` with two columns, the origin state (`\"Origin\"`) and outdegree value (`\"Outdegree\"`), sorted in descending order of outdegree. (You should be able to understand this code, which may help you in the next exercise.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "PjAi8zBGzSnJ",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def calc_outdegrees(flights):\n",
    "    outdegrees = flights[['ORIGIN_STATE_ABR', 'DEST_STATE_ABR']] \\\n",
    "                 .groupby(['ORIGIN_STATE_ABR']) \\\n",
    "                 .count() \\\n",
    "                 .reset_index() \\\n",
    "                 .rename(columns={'ORIGIN_STATE_ABR': 'Origin',\n",
    "                                  'DEST_STATE_ABR': 'Outdegree'}) \\\n",
    "                 .sort_values(by='Outdegree', ascending=False) \\\n",
    "                 .reset_index(drop=True)\n",
    "    return outdegrees\n",
    "\n",
    "# Demo:\n",
    "outdegrees = calc_outdegrees(flights)\n",
    "print(f\"There are {len(outdegrees)} states with a non-zero outdegree.\")\n",
    "print(\"Here are the first ten:\")\n",
    "outdegrees.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "RW9IUD02zSnK",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Exercise 4 (2 points): State transition probabilities ###\n",
    "\n",
    "To run the ranking analysis, recall that we need to construct a probability transition matrix. For our state-to-state analysis, we therefore wish to estimate the probability of going from state $i$ to state $j$. Let's define that  probability to be the number of direct flight segments from state $i$ to state $j$ divided by the outdegree of state $i$.\n",
    "\n",
    "Complete the function, `calc_state_trans_probs(flights, outdegrees)` to compute these state-to-state transition probabilities. Your function should accept two data frames like `flights` and `outdegrees` as defined above. In particular, you may assume the following;\n",
    "\n",
    "- The `flights` data frame has three columns: `\"ORIGIN_STATE_ABR\"` (originating state, a two-letter abbreviation), `\"DEST_STATE_ABR\"` (destination state abbreviation), and `\"FL_DATE\"` (date of direct flight).\n",
    "- The `outdegrees` data frame has two columns: `\"Origin\"` (originating state, a two-letter abbreviation) and `\"Outdegree\"` (an integer).\n",
    "\n",
    "Your function should and return a new data frame with exactly these columns:\n",
    "\n",
    "- `\"Origin\"`: The origin state, i.e., state $i$, as a two-letter abbreviation.\n",
    "- `\"Dest\"`: The destination state, i.e., state $j$, as a two-letter abbreviation.\n",
    "- `\"Count\"`: The number of direct flight segments from state $i$ to state $j$.\n",
    "- `\"TransProb\"`: The transition probability of going from state $i$ to state $j$, i.e., the count divided by the outdegree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "OkxYwCuqzSnK"
   },
   "outputs": [],
   "source": [
    "def calc_state_trans_probs(flights, outdegrees):\n",
    "    ###\n",
    "    ### YOUR CODE HERE\n",
    "    ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "RbLFVG50zSnL",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Demo, Part 0:\n",
    "probs = calc_state_trans_probs(flights, outdegrees)\n",
    "# print(f\"There are {len(probs)} state-to-state transition probabilities in your result.\")\n",
    "# print(\"Here are ten with the largest transition probabilities:\")\n",
    "# display(probs.sort_values(by=\"TransProb\", ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "RTvwGZB7zSnM",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Demo, Part 1:\n",
    "\n",
    "print(\"\"\"\n",
    "As a sanity check, let's see if the sum of all outgoing links per state\n",
    "is (approximately) 1.0. If it isn't, meaning any of the rows of the\n",
    "output below are `False`, use that information to help yourself debug.\n",
    "\"\"\")\n",
    "sanity = (probs[['Origin', 'TransProb']].groupby('Origin').sum() - 1.0).abs() < 1e-14\n",
    "sanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "UiJe-F4PzSnN",
    "nbgrader": {
     "grade": true,
     "grade_id": "ex4__calc_state_trans_probs",
     "locked": true,
     "points": "2",
     "solution": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test cell: `ex4__calc_state_trans_probs` (2 points)\n",
    "\n",
    "def ex4_gen_df_st(st):\n",
    "    from random import randint, random, choice\n",
    "    from collections import defaultdict\n",
    "    from problem_utils import ex0_random_date\n",
    "    from pandas import DataFrame\n",
    "    states = list(STATE_NAMES[\"Abbrv\"])\n",
    "    num_unique_edges = randint(1, 4)\n",
    "    dates = []\n",
    "    dests = []\n",
    "    counts = defaultdict(int)\n",
    "    outdegree = 0\n",
    "    for _ in range(num_unique_edges):\n",
    "        if random() < 0.33:\n",
    "            num_reps = randint(2, 4)\n",
    "        else:\n",
    "            num_reps = 1\n",
    "        dest_st = choice(states)\n",
    "        dests += [dest_st] * num_reps\n",
    "        dates += [ex0_random_date() for _ in range(num_reps)]\n",
    "        counts[(st, dest_st)] += num_reps\n",
    "        outdegree += num_reps\n",
    "    flights = DataFrame({\"FL_DATE\": dates,\n",
    "                         \"ORIGIN_STATE_ABR\": [st] * len(dates),\n",
    "                         \"DEST_STATE_ABR\": dests})\n",
    "    dests_st = []\n",
    "    counts_st = []\n",
    "    probs_st = []\n",
    "    for (st, dest_st), c in counts.items():\n",
    "        dests_st.append(dest_st)\n",
    "        counts_st.append(c)\n",
    "        probs_st.append(c / outdegree)\n",
    "    sts = [st] * len(dests_st)\n",
    "    probs = DataFrame({\"Origin\": sts,\n",
    "                       \"Dest\": dests_st,\n",
    "                       \"Count\": counts_st,\n",
    "                       \"TransProb\": probs_st})\n",
    "    return flights, probs, outdegree\n",
    "\n",
    "def ex4_check_one():\n",
    "    from random import randint, sample\n",
    "    from pandas import DataFrame, concat\n",
    "    num_states = randint(1, 4)\n",
    "    states = list(STATE_NAMES[\"Abbrv\"])\n",
    "    flights_list = []\n",
    "    probs_list = []\n",
    "    outdegrees_list = []\n",
    "    sts = sample(states, num_states)\n",
    "    for st in sts:\n",
    "        flights_st, probs_st, outdegree_st = ex4_gen_df_st(st)\n",
    "        flights_list.append(flights_st)\n",
    "        probs_list.append(probs_st)\n",
    "        outdegrees_list.append(outdegree_st)\n",
    "    flights = concat(flights_list, ignore_index=True) \\\n",
    "              .sort_values(by=\"FL_DATE\") \\\n",
    "              .reset_index(drop=True)\n",
    "    probs = concat(probs_list, ignore_index=True) \\\n",
    "            .sort_values(by=\"Origin\") \\\n",
    "            .reset_index(drop=True)\n",
    "    outdegrees = DataFrame({\"Origin\": sts,\n",
    "                            \"Outdegree\": outdegrees_list})\n",
    "    try:\n",
    "        your_probs = calc_state_trans_probs(flights, outdegrees)\n",
    "        assert_tibbles_are_equivalent(probs, your_probs)\n",
    "    except:\n",
    "        print(\"\\n*** ERROR ***\\n\")\n",
    "        print(\"`flights` input:\")\n",
    "        display(flights)\n",
    "        print(\"`outdegrees` input:\")\n",
    "        display(outdegrees)\n",
    "        print(\"Expected output:\")\n",
    "        display(probs)\n",
    "        print(\"Your output:\")\n",
    "        display(your_probs)\n",
    "        raise\n",
    "    \n",
    "for trial in range(10):\n",
    "    print(f\"=== Trial #{trial} / 9 ===\")\n",
    "    ex4_check_one()\n",
    "\n",
    "EXERCISE4_PASSED = True\n",
    "print(\"\\n(Passed.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "0r6dIAt2zSnO",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### (In case Exercise 4 isn't working) ###\n",
    "\n",
    "The rest of this notebook completes the comparison between state-rankings by confirmed cases and those by the airport network. It does depend on a working Exercise 4. However, running it is for your edification only, as there are no additional exercises or test cells below. Nevertheless, if the autograder has trouble completing due to errors in the code below, you can try converting the code cells to Markdown (effectively disabling them) and see if that helps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "RpAXqzyyzSnO",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**State rankings.** The next code cell runs the PageRank-style algorithm on the state-to-state airport network and produces a ranking. It depends on a correct result for Exercise 4, so if yours is not working completely, it might not run to completion. If that causes issues with the autograder, you can try converting the cell to Markdown to (effectively) disable it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RJJgvIRMzSnP",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def spy(A, figsize=(6, 6), markersize=0.5):\n",
    "    \"\"\"Visualizes a sparse matrix.\"\"\"\n",
    "    from matplotlib.pyplot import figure, spy, show\n",
    "    fig = figure(figsize=figsize)\n",
    "    spy(A, markersize=markersize)\n",
    "    show()\n",
    "    \n",
    "def display_vec_sparsely(x, name='x'):\n",
    "    from numpy import argwhere\n",
    "    from pandas import DataFrame\n",
    "    i_nz = argwhere(x).flatten()\n",
    "    df_x_nz = DataFrame({'i': i_nz, '{}[i] (non-zero only)'.format(name): x[i_nz]})\n",
    "    display(df_x_nz.head(5))\n",
    "    if len(df_x_nz) > 5:\n",
    "        print(\"...\")\n",
    "        display(df_x_nz.tail(5))\n",
    "\n",
    "def eval_markov_chain(P, x0, t_max):\n",
    "    x = x0\n",
    "    for t in range(t_max):\n",
    "        x = P.T.dot(x)\n",
    "    return x\n",
    "\n",
    "def rank_states_by_air_network(probs, t_max=100, verbose=True):\n",
    "    from numpy import array, zeros, ones, argsort, arange\n",
    "    from scipy.sparse import coo_matrix\n",
    "    from pandas import DataFrame\n",
    "\n",
    "    # Create transition matrix\n",
    "    unique_origins = set(probs['Origin'])\n",
    "    unique_dests = set(probs['Dest'])\n",
    "    unique_states = array(sorted(unique_origins | unique_dests))\n",
    "    state_ids = {st: i for i, st in enumerate(unique_states)}\n",
    "    num_states = max(state_ids.values()) + 1\n",
    "    \n",
    "    s2s = probs.copy()\n",
    "    s2s['OriginID'] = s2s['Origin'].map(state_ids)\n",
    "    s2s['DestID'] = s2s['Dest'].map(state_ids)\n",
    "    \n",
    "    P = coo_matrix((s2s['TransProb'], (s2s['OriginID'], s2s['DestID'])),\n",
    "                   shape=(num_states, num_states))\n",
    "    if verbose: spy(P)\n",
    "\n",
    "    # Run ranking algorithm\n",
    "    x0 = zeros(num_states)\n",
    "    x0[state_ids['WA']] = 1.0 # First state to report confirmed COVID-19 cases\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Initial condition:\")\n",
    "        display_vec_sparsely(x0, name='x0')\n",
    "        \n",
    "    x = eval_markov_chain(P, x0, t_max)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Final probabilities:\")\n",
    "        display_vec_sparsely(x)\n",
    "        \n",
    "    # Produce a results table of rank-ordered states\n",
    "    ranks = argsort(-x)\n",
    "    df_ranks = DataFrame({'Rank': arange(1, len(ranks)+1),\n",
    "                          'State': unique_states[ranks],\n",
    "                          'x(t)': x[ranks]})\n",
    "    df_ranks['ID'] = df_ranks['State'].map(state_ids)\n",
    "        \n",
    "    return df_ranks\n",
    "\n",
    "if \"EXERCISE4_PASSED\" in dir() and EXERCISE4_PASSED:\n",
    "    print(\"Running the ranking algorithm...\")\n",
    "    airnet_rankings = rank_states_by_air_network(probs, verbose=False)\n",
    "\n",
    "    print(f\"==> Here are the top-{TOP_K} states:\")\n",
    "    display(airnet_rankings.head(TOP_K))\n",
    "else:\n",
    "    print(\"We did not detect that the Exercise 4 test cell passed, so we aren't running this cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "gtTGIhm8zSnQ",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**Comparing the two rankings.** We now have a ranking of states by number of confirmed COVID-19 cases, as well as a separate ranking of states by air-network connectivity. To compare them, we'll use a measure called [_rank-biased overlap_ (RBO)](https://doi.org/10.1145/1852102.1852106). Very roughly speaking, this measure is an estimate of the probability that a reader comparing the top few entries of two rankings tends to encounter the same items, so a value closer to 1 means the top entries of the two rankings are more similar.\n",
    "\n",
    "> **Note 0.** We say \"top few\" above because RBO is parameterized by a \"patience\" parameter, which is related to how many of the top entries the reader will inspect before stopping. The reason for this parameter originates in the motivation for RBO, which was to measure the similarity between search engine results. The code we are using to calculate RBO uses [this implementation](https://github.com/dlukes/rbo)).\n",
    ">\n",
    "> **Note 1.** This cell should only be run if Exercise 4 passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SQlgN5ElzSnR",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from rbo import rbo\n",
    "\n",
    "if \"EXERCISE4_PASSED\" in dir() and EXERCISE4_PASSED:\n",
    "    compare_rankings = rbo(covid19_rankings, # ranking by confirmed COVID-19 cases\n",
    "                           airnet_rankings['State'].values, # ranking by air-network connectivity\n",
    "                           0.95) # \"patience\" parameter\n",
    "    print(f\"Raw RBO result: {compare_rankings}\\n\\n==> RBO score is {compare_rankings.ext:.3}\")\n",
    "else:\n",
    "    print(\"We did not detect that the Exercise 4 test cell passed, so we aren't running this cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "MzA72tK-zSnS",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "If everything is correct, you'll see an RBO score of around 0.6, which suggests that the connectivity of the airport network may help explain the number of confirmed COVID-19 cases we are seeing in each state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "_egsCN8TzSnS",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**Fin!** Youve reached the end of this problem. Dont forget to restart and run all cells again to make sure its all working when run in sequence; and make sure your work passes the submission process. Good luck!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "problem18.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
